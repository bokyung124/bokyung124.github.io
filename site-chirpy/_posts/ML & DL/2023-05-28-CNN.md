---
title: "[BoostCourse] 4. Regularization"
last_modified_at: 2023-05-27T22:20:00-05:00
layout: post
categories:
    - ML & DL
toc: true
toc_sticky: true
author_profile: true
mathjax: true
published: true
tag: [study, Boostcourse, DL]
---

<https://www.boostcourse.org/ai111>

<br>

## Convolution

- Continuous convolution   
$(f*g)(t) = \int f(\tau)g(t-\tau)d\tau = \int f(t-\tau)g(t)d\tau$   

<br>

- Discrete convolution   
$(f*g)(t) = \sum_{i=-\infty}^{\infty} f(i)g(t-i) = \sum_{i=-\infty}^{\infty} f(t-i)g(i)$   

<br>

- 2D image convolution   
$(I*K)(i, j) = \sum_{m}\sum_{n} I(m,n)K(i-m,j-n) = \sum_{m}\sum_{n} I(i-m,i-n)K(m,n)$

<br>

<img width="320" alt="스크린샷 2023-05-28 오후 11 07 20" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/f6e49ece-82fb-4981-8d04-d52fb947287d">

<img width="364" alt="스크린샷 2023-05-28 오후 11 07 35" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/dcf7cbf7-d43f-43c4-8de1-f43c6f78a228">

- 3x3 convolution filter를 7x7 image에 찍으면 해당 위치에 있는 convolution filter값과 이미지의 픽셀값을 곱해서 더하면 output의 $O_{11}$ 값이 됨

- Blur, Emboss, Outline 등에 활용

<br>

### RGB image convolution

<img width="312" alt="스크린샷 2023-05-28 오후 11 30 57" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/0b016f55-6129-41fb-8d39-c105acdefc94">

<img width="347" alt="스크린샷 2023-05-28 오후 11 31 33" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/d9ffd1f2-65b8-4fb6-8f9c-066fd9b16ee0">

- 여러 개의 채널을 갖는 convolution feature map 생성됨
    - 여러 개의 convolution filters
    - input channel과 output channel의 크기를 알면, 적용된 convolution filter의 개수 알 수 있음

<br>

### stack of convolutions

<img width="317" alt="스크린샷 2023-05-28 오후 11 33 00" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/e616c1b8-820e-4778-9a7a-594b60c24126">

- 한 번 convolution을 거치고 나면 nonlinear activition이 들어가게 됨
- 이 연산에 필요한 파라미터의 숫자를 항상 생각해야 함!

<br>

## Convolution Neural Networks

- CNN consists of **convoluiton layer**, **pooling layer**, and **fully connected layer**
    - convolution & pooling layers: feature extraction
    - fully connected layer: decision making (e.g., classification)

<img width="332" alt="스크린샷 2023-05-28 오후 11 38 17" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/07cbec0a-0f8e-4818-9892-65ae9eb19cce">

<br>

- 최근에는 fully connected layer를 없애는/최소화하는 추세
    - ML에서 일반적으로 학습하고자 하는 모델의 파라미터의 숫자가 늘어날수록 학습이 어렵고 generalization performance가 떨어진다고 알려져 있음
    - CNN - 같은 모델을 만들고 convolution layer를 많이 추가하여 deep하게 만들지만, 동시에 파라미터 수를 줄이는 데 집중

<br>

### Convolution Arithmetic of GoogleNet

<img width="409" alt="스크린샷 2023-05-28 오후 11 41 17" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/3940651a-7664-484c-8971-077e666d0dbe">

<br>

## Stride

<img width="440" alt="스크린샷 2023-05-28 오후 11 42 23" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/12c72f18-7fc6-43d0-9900-d868fd568d12">

- 갖고 있는 convolution filter(kernel)을 몇 칸씩 옮기느냐

<br>

## Padding

<img width="169" alt="스크린샷 2023-05-29 오전 1 25 20" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/e80a2ebc-86b7-4d4b-bda6-a719ca58434a">

<img width="463" alt="스크린샷 2023-05-29 오전 1 27 45" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/ba72f55a-f968-4092-9c23-b8bd46c8c62f">

<br>

## Convoluiton Arithmetic

<img width="435" alt="스크린샷 2023-05-29 오전 1 28 18" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/1a2255a9-de60-4200-b3d5-df3642c20e1f">

- 필요한 파라미터 수: 3 x 3 x 128 x 64 = 73,728

<br>

### exercise

- AlexNet

<img width="414" alt="스크린샷 2023-05-29 오전 1 33 25" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/db3d9d33-11d6-4f93-aa2b-3fe490143ab9">

- Convolution 레이어(1): 11 x 11 x 3 x 48 * 2 ≈ 35K
- Convolution 레이어(2): 5 x 5 x 28 x 128 * 2 ≈ 307K
- Convolution 레이어(3): 3 x 3 x 128 * 2 x 192 * 2 ≈ 884K
- Convolution 레이어(4): 3 x 3 x 192 x 192 * 2 ≈ 663K
- Convolution 레이어(5): 3 x 3 x 192 x 128 * 2 ≈ 442K 

- Dense 레이어(1): 13 * 13 * 128 * 2 x 2048 * 2 ≈ 177M
    - fully connected -> input 뉴런 개수 x output 뉴런 개수
- Dense 레이어(2): 2048 * 2 x 2048 * 2 ≈ 16M
- Dense 레이어(3): 2048 * 2 x 1000 ≈ 4M

<br>

<img width="416" alt="스크린샷 2023-05-29 오전 1 41 04" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/bc7a28c5-5dca-4410-87e8-43978a703d9e">

<br>

- Dense 레이어가 Convolution 레이어보다 훨씬 많은 파라미터 가짐
    - Convoluiton operator와 각각의 커널이 모든 위치에 대해 동일하게 적용됨
        - shared parameter
    - 파라미터 수를 줄이기 위해 convolution 레이어를 늘리고, fully connected 레이어를 줄이는 추세

<br>

## 1 X 1 Convolution

<img width="206" alt="스크린샷 2023-05-29 오전 1 44 23" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/bdc4a107-97a2-4efb-9850-d33108f6e684">

- 이유
    - Dimension reduction (Dimension: channel)
    - convolution layer를 깊게 쌓으면서 파라미터를 줄일 수 있음
    - e.g., bottleneck architecture

<br>

## AlexNet

<img width="412" alt="스크린샷 2023-05-29 오전 1 51 15" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/9104a20b-4782-4052-93a7-beaf399a1931">

<br>

- key ideas
    - ReLU (Rectified Linear Unit) activation
    - GPU implementation (2 GPUs)
    - Local response normalization (지금 많이 사용되지 않음), Overlapping pooling
    - Data augmentation
    - Dropout

<br>

- ReLU activation   
    <img width="144" alt="스크린샷 2023-05-29 오전 1 54 55" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/d77d5441-7d88-4d3f-be50-0cc5f9982b0a">        
    - linear model의 장점을 갖는 nonlinear
    - gradient descent로 학습 용이
    - good generalization
    - vanishing gradient 문제 해결

<br>

## VGGNet

<img width="228" alt="스크린샷 2023-05-29 오전 1 55 35" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/40d129ff-7cc3-4bf8-a4ef-7cef35e069ac">

- **3 X 3** convolution filter만 사용
- fully connected layer에 1 x 1 covolution 사용

<br>

- 3 X 3 convolution
    <img width="346" alt="스크린샷 2023-05-29 오전 1 56 49" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/65c028f4-a7b8-455e-aa2e-ed03f528d6b5">
    - convolution layer의 크기가 커지면 한 번 찍을 때 고려되는 input의 크기가 커짐 : **Receptive filed**


<br>

## GoogLeNet

- 1 X 1 convolution을 적절히 활용함
    - parameter 수 감소

- Inception Block

<img width="374" alt="스크린샷 2023-05-29 오전 2 01 46" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/58aac01c-6905-4dd2-8bec-1719f4ee82c5">


<br>

## ResNet

<img width="414" alt="스크린샷 2023-05-29 오전 2 11 06" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/dc6101a7-32ee-4084-8ae6-96d2fd200db6">