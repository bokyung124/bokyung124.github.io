---
title: "[pyspark] 기본 문법"
last_modified_at: 2023-05-02T16:17:00-05:00
layout: post
categories:
    - Spark
toc: true
toc_sticky: true
author_profile: true
mathjax: true
published: true
tag: [study, Spark]
---

<br>

## SparkSession


```python
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('Test').getOrCreate()
spark.conf.sest('spark.sql.execution.arrow.pyspark.enabled', 'true')    # 메모리 관련
```

<br>

## read csv


```python
sparkDF = spark.read.csv('hdfs://localhost:9000/housing.csv', encoding='cp949', header=True, inferSchema=True)
sparkDF = spark.read.option('header', 'true').option('encoding', 'cp949').csv('hdfs://localhost:9000/housing.csv', inferSchema=True)

sparkDF.show()
```

<br>

<img width="1081" alt="image" src="https://user-images.githubusercontent.com/53086873/235630259-56e1f3bf-711a-4334-88dc-3df5def90090.png">

<br>

```python
emp = spark.read.option('encoding','utf-8').option('header','true').csv("../data/Employee.csv",inferSchema=True)
```

### inferSchema=True
- 변수들의 datatype까지 가져옴
- `True`로 설정하지 않으면 모두 String타입

```python
emp.printSchema()
```

<img width="325" alt="image" src="https://user-images.githubusercontent.com/53086873/235631076-96ce4f00-b659-4083-b466-c03682ea458a.png">

<br>

## Hadoop -> Pandas


```python
house = sparkDF.toPandas()
```

<br>

## DataFrame 출력


### 상위 20개 데이터 출력

```python 
emp.show()
```
- df 형태는 모두 `.show()`를 붙여야 df 형태로 출력됨!

<br>

### 특정 컬럼만 출력

```python
emp.select('id').show()
emp.select(['id', 'gender']).show()
```

<img width="199" alt="image" src="https://user-images.githubusercontent.com/53086873/235632911-2abcca3e-abb0-4c88-99ba-64a9f94eaaff.png">

<br>

<img width="197" alt="image" src="https://user-images.githubusercontent.com/53086873/235633006-e6981f76-c3cd-4bcb-afd8-0664099a8fd7.png">

<br>

#### 기타

```python
emp.head()
```

<img width="959" alt="image" src="https://user-images.githubusercontent.com/53086873/235634007-a82b999d-45bb-4bd7-8bb0-7c0ac63a4292.png">

<br>

```python
emp['gender']
# .show() 사용 불가
```

Column<'gender'>

<br>

```python
print(type(emp.select('id')),type(emp['id']))
```

<class 'pyspark.sql.dataframe.DataFrame'> <class 'pyspark.sql.column.Column'>

<br>

## describe()


```python
emp.describe().show()
```

<br>

## 컬럼 


### 컬럼 추가

```python
emp = emp.withColumn('jobtime2', emp['jobtime']*2)
```
- jobtime 컬럼의 값을 2배한 값을 갖는 jobtime2 컬럼 생성

<br>

### 컬럼명 변경

```python
emp = emp.withColumnRenamed('jobtime2', 'jobtime3')
```
- jobtime2 컬럼명 -> jobtime3로 변경

<br>

### 컬럼 삭제

```python
emp = emp.drop('jobtime3')
```
- jobtime3 컬럼 삭제

<br>

## Filter Operations

`&`, `|`, `~`


```python
emp.filter('salary <= 30000').show()
emp.where('salary <= 30000').show()

emp.filter('salary <= 50000').select(['gender', 'jobcat']).show()

emp.filter(emp['salary'] <= 30000).show()

emp.filter((emp['salary'] <= 30000) & (emp['salary'] >= 25000)).show()

emp.filter(~(emp['salary'] <= 30000)).show()
```

<br>

## groupBy, .agg()


```python
emp.groupBy('gender').count().show()                       # 그룹별 카운트
 
emp.groupBy('gender').mean().show()                        # 그룹별 평균

emp.groupBy('gender').mean('salary').show()

emp.groupBy(['gender', 'jobcat']).max().show()             # 그룹별 최댓값

emp.agg({'salary' : 'mean', 'salbegin' : 'min'}).show()    # 컬럼 별 수치값
```

<img width="131" alt="image" src="https://user-images.githubusercontent.com/53086873/235638375-7470b073-5c88-44b4-afa2-591dce9f806c.png">

<br>

<img width="960" alt="image" src="https://user-images.githubusercontent.com/53086873/235638496-e1d1b496-1cc2-4824-87b9-4e5645749363.png">

<br>

<img width="223" alt="image" src="https://user-images.githubusercontent.com/53086873/235638582-65fd8b41-0b5c-450e-9681-974dbf9d12fd.png">

<br>

<img width="679" alt="image" src="https://user-images.githubusercontent.com/53086873/235638687-358e093c-beb8-48fe-9f57-72271fd87a24.png">

<br>

<img width="267" alt="image" src="https://user-images.githubusercontent.com/53086873/235639205-6b69415b-097b-4cc0-8382-83593b69ca7d.png">

<br>

## 정렬


```python
emp.orderBy('salary', ascending=False).show()

emp.orderBy('educ', 'salary', ascending=[False, True]).show()
```

<br>

## 데이터프레임 복사


```python
emp_copy = emp.select('*')
```

<br>

## pyspark built-in func.


```python
# 표준화
from pyspark.sql.functions import arg, col, stddev
sal_mean = emp_copy.select(avg(col('salary'))).first()[0]    
sal_std = emp_copy.select(std(col('salary'))).first()[0]
emp_copy.withColumn('salary_std', (col('salary') - sal_mean) / sal_std).show()    
```

- `.first()[0]` : 값만 가져옴
    - `emp_copy.select(avg(col('salary')))` 는 df 형태

<br>

## 데이터 저장 및 불러오기


### csv

<img width="485" alt="image" src="https://user-images.githubusercontent.com/53086873/235825379-8ec4e7a8-5962-4829-b659-a41b73bd0f75.png">

<br>

```python
spark = SparkSession.builder.appName('Test').getOrCreate()

# read
sparkDF = spark.read.csv("hdfs://localhost:9000/Spark/spark_Employee.csv")
sparkDF = 스파크.read.option('encoding','cp949').option('header','true').csv("hdfs://localhost:9000/Spark/spark_Employee.csv")

# write
sparkDF.write.csv("hdfs://localhost:9000/Spark/hadoop_Employee.csv")

# 폴더 생성하며 지정한 형태로 저장
data.coalesce(1).write.format("com.databricks.spark.csv").option("header", "true").save("hdfs://localhost:9000/Text/csv")
```

### pandas <-> spark

```python
# spark -> pandas
pandasDF_spark = sparkDF.toPandas()

# pandas -> spark
sparkDF_pd  = spark.createDataFrame(pandasDF)
```

### parquet

```python
spark = SparkSession.builder.appName('Test').getOrCreate()

# read
sparkPQ = spark.read.parquet("hdfs://localhost:9000/Spark/spark_Employee.parquet")

# write
sparkDF.write.parquet("hdfs://localhost:9000/Spark/spark_Employee.parquet")
```

#### cf) pandas

```python
# read csv
pandasDF = pd.read_csv('Employee.csv')
# write csv
pandasDF.to_csv('pandas_Employee.csv')
# read parquet
pandasPQ  = pd.read_parquet('pandas_Employee.parquet')
# write parquet
pandasDF.to_parquet('pandas_Employee.parquet')
```