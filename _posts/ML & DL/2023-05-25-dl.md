---
title: "[BoostCourse] 1. 딥러닝 기본"
last_modified_at: 2023-05-25T22:20:00-05:00
layout: post
categories:
    - ML & DL
toc: true
toc_sticky: true
author_profile: true
mathjax: true
published: true
---

<https://www.boostcourse.org/ai111>

<br>

## Introduction

- AI: Mimic human intelligence
- ML: Data-driven approach
- DL: Neural networks
- Deep Learning ⊂ Machine Learning ⊂ AI

<br>

### key components of Deep Learning
- the **data** that the model can learn from
- the **model** how to transform the data
- the **loss function** that quantifies the badness of the model
- the **algorithm** to adjust the parameters to minimize the loss

<br>

-> 새로운 논문을 읽을 때 이 네가지에 비춰 보면 이 논문/연구가 기존 연구에 비해 어떤 장점이 있고, 어떤 contribution이 있는지 이해하기 쉬움

<br>

## Neural Networks

- function approzimators that stack affine transformations followed by nonlinear transformations
- 함수를 근사한 모델
- 행렬 곱 / 비선형 연산(activation function)이 반복적으로 일어나는 모델

<br>

### linear function

<img width="223" alt="스크린샷 2023-05-26 오후 6 25 13" src="https://github.com/Superstar-project/DBP/assets/53086873/7a49637a-09b9-4b0f-a62a-cd55f0df9e3b">

- 입력 & 출력 1차원 -> 입력과 출력을 연결하는 모델을 찾는 것이 목표
- line의 기울기와 절편 -> 2개의 parameters

<br>

<img width="190" alt="스크린샷 2023-05-26 오후 6 26 19" src="https://github.com/Superstar-project/DBP/assets/53086873/85a84158-7200-4f4a-ac55-afcb3057ca63">

- 회귀문제 -> sqaured loss function 사용

<br>

<img width="216" alt="스크린샷 2023-05-26 오후 6 28 19" src="https://github.com/Superstar-project/DBP/assets/53086873/515f0380-11ac-451e-873c-2092194023d0">

- 목표: loss function을 줄이는 것
    - parameter가 어느 방향으로 움직였을 때 줄어드는지 찾고, 그 방향으로 parameter 바꿈
    - **back propagation**

<br>

- **partial derivative** 
    - N개의 target값과 예측값 사이의 제곱을 minimize하는 loss function의 `w`에 대한 편미분
    - `b`에 대해서도 편미분

- update
<img width="113" alt="스크린샷 2023-05-26 오후 6 31 46" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/65e745c5-5242-4ee7-acd9-d325b67900eb">

=> **Gradient descent**

<br>

## Beyond Linear Neural Networks

- linear layer를 계속 쌓으면 한 개의 layer와 다를 것이 없음    
-> **nonlinear transform** 필요

<img width="342" alt="스크린샷 2023-05-26 오후 6 36 43" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/ca3d5791-73be-43c9-9dab-d376910cbbe9">

- network의 표현력 극대화    
-> [선형결합 반복 + activation function 곱 (nonlinear transform)] n번 반복

<br>

### activation functions


- ReLU   
<img width="149" alt="스크린샷 2023-05-26 오후 6 38 48" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/9e1534ef-3e69-4a44-8486-e5a4a2344d0b">


- Sigmoid   
<img width="151" alt="스크린샷 2023-05-26 오후 6 39 04" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/b39e0f95-40e4-4398-abc1-ba3c063a090a">

- Hyperbolic Tangent   
<img width="146" alt="스크린샷 2023-05-26 오후 6 39 21" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/038068fd-cfc2-4e9b-9c2a-30d6e72ae28b">

<br>

- 상황마다 적절한 activation function은 다름

<br>

## Multi-Layer Perceptron

<img width="262" alt="스크린샷 2023-05-26 오후 6 41 53" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/bc71559b-ead6-404b-a110-317a333fbad4">

- 입력 -> linear/nonliner transform -> hidden layer -> affine transform 
    - 한 / 두 단 정도

<br>

### loss function

- Regression - `MSE`    
<img width="197" alt="스크린샷 2023-05-26 오후 6 43 29" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/bd310e45-2545-41ce-afb1-8831e122be49">

<br>

- Classification - `Cross Entropy`      
<img width="191" alt="스크린샷 2023-05-26 오후 6 45 08" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/69bc3fb4-d5ca-43e8-9e52-d1ad8c1634fd">

<br>

- Probablistic - `MLE (=MSE)`   
<img width="272" alt="스크린샷 2023-05-26 오후 6 48 28" src="https://github.com/bokyung124/bokyung124.github.io/assets/53086873/253b967d-42e0-4c5c-a520-759f21cd0bb1">